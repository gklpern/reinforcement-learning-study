![alt text](image.png)

Planning (Planlama):

Bir kararÄ±n sadece anlÄ±k getirisi deÄŸil, uzun vadeli etkileri de var.

Ã–rneÄŸin: â€œÅimdi tatlÄ± yemek mi, yoksa diyet mi yapmak?â€ â†’ kÄ±sa vadede tatlÄ± iyi hissettirir ama uzun vadede saÄŸlÄ±ÄŸa zararlÄ±.

Learning (Ã–ÄŸrenme):

Temporal Credit Assignment Problem: Hangi geÃ§miÅŸ kararÄ±m bugÃ¼nkÃ¼ Ã¶dÃ¼lÃ¼ getirdi?

Ã–rneÄŸin: 100 adÄ±m Ã¶nce aldÄ±ÄŸÄ±n anahtar yÃ¼zÃ¼nden bugÃ¼n kazandÄ±n. Ama algoritma nasÄ±l bilecek ki bu kadar eski bir aksiyonun bu sonucu doÄŸurduÄŸunu?

Bu RLâ€™in en temel teknik zorluklarÄ±ndan biridir.


# Markov Assumption

![alt text](image-1.png)

BazÄ± durumlarda Markov varsayÄ±mÄ± doÄŸru deÄŸildir.

Ã–rn: Bir Ã¶ÄŸrencinin performansÄ± â†’ sadece bugÃ¼nkÃ¼ bilgi deÄŸil, Ã¶nceki deneyimlere de baÄŸlÄ± olabilir.

BÃ¶yle durumlarda POMDP (Partially Observable Markov Decision Process) kullanÄ±lÄ±r.

ğŸ¯ Ã–zet

Markov varsayÄ±mÄ± RLâ€™in temel taÅŸÄ±.

Ã‡Ã¼nkÃ¼ bu sayede matematiksel olarak MDP (Markov Decision Process) tanÄ±mÄ± yapÄ±labiliyor.

Ajan: â€œSadece bugÃ¼nkÃ¼ stateâ€™i bilsem yeterâ€ diye dÃ¼ÅŸÃ¼nÃ¼r.

![alt text](image-2.png)


![alt text](image-3.png)


ğŸ”¹ Transition Model (GeÃ§iÅŸ modeli)

Bu ÅŸunu sÃ¶yler:

â€œÅu an buradaysam ve ÅŸunu yaparsam, sonraki durumda nerede olacaÄŸÄ±m?â€

Ã–rn:

Labirent oyunu: SaÄŸ tuÅŸuna bastÄ±n â†’ %80 ihtimalle saÄŸa gidersin, %20 ihtimalle kayarsÄ±n ve aÅŸaÄŸÄ± dÃ¼ÅŸersin.

ğŸ”¹ Reward Model (Ã–dÃ¼l modeli)

Bu ÅŸunu sÃ¶yler:

â€œÅu an buradayÄ±m ve ÅŸu aksiyonu yapÄ±yorum â†’ bundan ne kadar puan kazanÄ±rÄ±m?â€

Ã–rn:

Labirent oyunu: Ã‡Ä±kÄ±ÅŸ kapÄ±sÄ±na ulaÅŸÄ±rsan +10 puan, duvara Ã§arparsan âˆ’1 puan, boÅŸ adÄ±mda 0 puan.



![alt text](image-4.png)


SÃ¼per, bu slayt RLâ€™de Ã§ok kullanÄ±lan bir Ã¶rnek ortamÄ± aÃ§Ä±klÄ±yor: Mars Rover problemi. Hadi basitleÅŸtirerek parÃ§a parÃ§a anlatalÄ±m:

ğŸš€ Ortam

Roverâ€™Ä±n bulunduÄŸu 7 tane hÃ¼cre var: 
S1,S2,â€¦,S7
S
1
	â€‹

,S
2
	â€‹

,â€¦,S
7
	â€‹

.

Rover bir aksiyon seÃ§iyor: Ã¶rneÄŸin â€œsaÄŸa gitâ€ (TryRight).

ğŸ”¹ Reward (Ã¶dÃ¼l)

YukarÄ±daki kutularÄ±n iÃ§inde â€œ
r^=0
r
^
=0â€ yazÄ±yor.

Yani burada gÃ¶sterilen tÃ¼m stateâ€™lerde anlÄ±k Ã¶dÃ¼l 0.

(Ama aslÄ±nda genelde bazÄ± stateâ€™lerde Ã¶dÃ¼l olur: mesela Ã§Ä±kÄ±ÅŸta +10, tehlikede âˆ’1).

ğŸ”¹ Transition Model (GeÃ§iÅŸ olasÄ±lÄ±klarÄ±)

Slaytta verilen Ã¶rnek:

0.5=P(s1âˆ£s1,TryRight)=P(s2âˆ£s1,TryRight)
0.5=P(s
1
	â€‹

âˆ£s
1
	â€‹

,TryRight)=P(s
2
	â€‹

âˆ£s
1
	â€‹

,TryRight)

Bunun anlamÄ±:

EÄŸer Rover 
S1
S
1
	â€‹

â€™deyken â€œsaÄŸa gitâ€ dersen, %50 ihtimalle aynÄ± yerde kalÄ±yor (
S1
S
1
	â€‹

)

%50 ihtimalle saÄŸa geÃ§iyor (
S2
S
2
	â€‹

).

Benzer ÅŸekilde:

0.5=P(s2âˆ£s2,TryRight)=P(s3âˆ£s2,TryRight)
0.5=P(s
2
	â€‹

âˆ£s
2
	â€‹

,TryRight)=P(s
3
	â€‹

âˆ£s
2
	â€‹

,TryRight)

Yani 
S2
S
2
	â€‹

â€™deyken saÄŸa git â†’ %50 ihtimalle 
S2
S
2
	â€‹

â€™de kal, %50 ihtimalle 
S3
S
3
	â€‹

â€™e geÃ§.

ğŸ”¹ Ã–nemli Nokta

Bu bir stochastic (olasÄ±lÄ±klÄ±) ortam â†’ her aksiyon aynÄ± sonucu vermez.

AjanÄ±n â€œdÃ¼nyayÄ± yanlÄ±ÅŸ modellemeâ€ ihtimali var. Yani Rover sanÄ±yor ki hep saÄŸa gidecek, ama aslÄ±nda yarÄ± yarÄ±ya kayÄ±yor.

ğŸ§© Basit benzetme

DÃ¼ÅŸÃ¼n: Marsâ€™ta rover kum tepelerinde ilerliyor. â€œSaÄŸa gitâ€ diyorsun â†’ ama zeminin kayganlÄ±ÄŸÄ±ndan dolayÄ± bazen yerinde sayÄ±yor, bazen gerÃ§ekten ilerliyor.



![alt text](image-6.png)








![alt text](image-5.png)